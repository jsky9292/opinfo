#!/usr/bin/env python3
"""
dkxm8.com í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨ ì™„ì „ í¬ë¡¤ëŸ¬
- ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ì—¬ ì „ì²´ ì—…ì²´ ìˆ˜ì§‘
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
from datetime import datetime
import re

# ë¡œê·¸ì¸ ì •ë³´
USERNAME = "mix1220"
PASSWORD = "1234"

# í…ŒìŠ¤íŠ¸ìš© ì§€ì—­ (ëŒ€ì „ë§Œ ë¨¼ì €)
TEST_REGIONS = {
    "ëŒ€ì „/ì¶©ì²­": ["ëŒ€ì „"]
}

# ì „ì²´ ì§€ì—­ ë¦¬ìŠ¤íŠ¸
ALL_REGIONS = {
    "ëŒ€ì „/ì¶©ì²­": ["ëŒ€ì „", "ì²œì•ˆ", "ì²­ì£¼", "ì„¸ì¢…", "ì„œì‚°", "ë‹¹ì§„", "ë³´ë ¹", "ì§„ì²œ", "ì•„ì‚°", "ì¶©ì£¼", "ì œì²œ", "í™ì„±", "ë…¼ì‚°", "ì˜¤ì°½", "ìŒì„±", "ì˜¥ì²œ", "ê³µì£¼", "ì¦í‰", "ê³„ë£¡", "ë¶€ì—¬", "íƒœì•ˆ", "ì˜ˆì‚°", "ì˜¤ì†¡"],
    "ë¶€ì‚°/ê²½ë‚¨": ["ë¶€ì‚°", "ìš¸ì‚°", "ì–‘ì‚°", "ë§ˆì‚°", "ì°½ì›", "ì§„í•´", "ê¹€í•´", "ê±°ì œ", "ì§„ì£¼", "í†µì˜", "ì‚¬ì²œ", "ë°€ì–‘", "í•¨ì•ˆ", "ì°½ë…•", "ê³ ì„±", "ë‚¨í•´", "í•˜ë™", "ì‚°ì²­", "í•¨ì–‘", "ê±°ì°½", "í•©ì²œ"],
    "ì„œìš¸/ê°•ë‚¨": ["ê°•ë‚¨êµ¬", "ì„œì´ˆêµ¬", "ì†¡íŒŒêµ¬", "ê°•ë™êµ¬", "ê´‘ì§„êµ¬", "ì„±ë™êµ¬", "ì¤‘êµ¬", "ì¢…ë¡œêµ¬", "ìš©ì‚°êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì€í‰êµ¬", "ê°•ì„œêµ¬", "ì–‘ì²œêµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ì˜ë“±í¬êµ¬", "ë™ì‘êµ¬", "ê´€ì•…êµ¬", "ê°•ë¶êµ¬", "ë…¸ì›êµ¬"],
    "ì¸ì²œ/ê²½ê¸°": ["ì¸ì²œ", "ì†¡ë„", "ë¶€í‰", "ê³„ì–‘", "ìˆ˜ì›", "ì„±ë‚¨", "ê³ ì–‘", "ìš©ì¸", "ë¶€ì²œ", "ì•ˆì‚°", "ì•ˆì–‘", "ë‚¨ì–‘ì£¼", "í™”ì„±", "í‰íƒ", "ì˜ì •ë¶€", "ì‹œí¥", "íŒŒì£¼", "ê¹€í¬", "ê´‘ëª…", "ê´‘ì£¼", "êµ°í¬", "í•˜ë‚¨"],
    "ëŒ€êµ¬/ê²½ë¶": ["ëŒ€êµ¬", "ì¤‘êµ¬", "ë™êµ¬", "ì„œêµ¬", "ë‚¨êµ¬", "ë¶êµ¬", "ìˆ˜ì„±êµ¬", "ë‹¬ì„œêµ¬", "í¬í•­", "ê²½ì£¼", "ê¹€ì²œ", "ì•ˆë™", "êµ¬ë¯¸", "ì˜ì£¼", "ì˜ì²œ", "ìƒì£¼", "ë¬¸ê²½", "ê²½ì‚°", "êµ°ìœ„", "ì˜ì„±", "ì²­ì†¡", "ì˜ì–‘", "ì˜ë•", "ì²­ë„", "ê³ ë ¹", "ì„±ì£¼", "ì¹ ê³¡", "ì˜ˆì²œ", "ë´‰í™”", "ìš¸ì§„", "ìš¸ë¦‰ë„"],
    "ê´‘ì£¼/ì „ë¼": ["ê´‘ì£¼", "ì „ì£¼", "êµ°ì‚°", "ìµì‚°", "ì •ì", "ë‚¨ì›", "ê¹€ì œ", "ëª©í¬", "ì—¬ìˆ˜", "ìˆœì²œ", "ë‚˜ì£¼", "ê´‘ì–‘"],
    "ê°•ì›/ì œì£¼/ì „ë¼": ["ì¶˜ì²œ", "ì›ì£¼", "ê°•ë¦‰", "ë™í•´", "íƒœë°±", "ì†ì´ˆ", "ì‚¼ì²™", "ì œì£¼", "ì„œê·€í¬"]
}


class PaginationCrawler:
    def __init__(self, headless=True, test_mode=True):
        self.headless = headless
        self.test_mode = test_mode
        self.driver = None
        self.results = []

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        print("ğŸ”§ ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")

        chrome_options = Options()

        if self.headless:
            chrome_options.add_argument('--headless=new')

        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )

        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.set_page_load_timeout(30)

            self.driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            self.driver.execute_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            )

            print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ")
            return True

        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def login(self):
        """ë¡œê·¸ì¸ ìˆ˜í–‰"""
        print("\nğŸ”‘ ë¡œê·¸ì¸ ì¤‘...")

        try:
            self.driver.get("https://dkxm8.com/bbs/login.php")
            time.sleep(3)

            username_field = self.driver.find_element(By.NAME, "mb_id")
            username_field.clear()
            username_field.send_keys(USERNAME)

            password_field = self.driver.find_element(By.NAME, "mb_password")
            password_field.clear()
            password_field.send_keys(PASSWORD)

            submit_btn = self.driver.find_element(By.CSS_SELECTOR, "input[type='submit']")
            submit_btn.click()

            time.sleep(5)

            if 'login.php' in self.driver.current_url:
                print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨")
                return False

            print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            return True

        except Exception as e:
            print(f"âŒ ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
            return False

    def get_total_pages(self, area_name):
        """í•´ë‹¹ ì§€ì—­ì˜ ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""
        try:
            # í˜ì´ì§€ë„¤ì´ì…˜ ì°¾ê¸° (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì‹œë„)
            pagination_selectors = [
                "div.pg_wrap",
                "div.pagination",
                "div[class*='paging']",
                "div[class*='page']",
                ".pg",
                "#pagination"
            ]

            max_page = 1

            for selector in pagination_selectors:
                try:
                    pagination = self.driver.find_element(By.CSS_SELECTOR, selector)

                    # í˜ì´ì§€ ë§í¬ì—ì„œ ìˆ«ì ì¶”ì¶œ
                    page_links = pagination.find_elements(By.TAG_NAME, "a")

                    for link in page_links:
                        try:
                            # hrefì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                            href = link.get_attribute('href')
                            if href and 'page=' in href:
                                page_match = re.search(r'page=(\d+)', href)
                                if page_match:
                                    page_num = int(page_match.group(1))
                                    max_page = max(max_page, page_num)

                            # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ
                            text = link.text.strip()
                            if text.isdigit():
                                page_num = int(text)
                                max_page = max(max_page, page_num)
                        except:
                            continue

                    if max_page > 1:
                        break

                except:
                    continue

            return max_page

        except Exception as e:
            print(f"   âš ï¸ í˜ì´ì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            return 1

    def crawl_page(self, area_name, region_name, page_num):
        """íŠ¹ì • ì§€ì—­ì˜ íŠ¹ì • í˜ì´ì§€ í¬ë¡¤ë§"""
        items = []

        try:
            # URL êµ¬ì„± (í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì¶”ê°€)
            if page_num == 1:
                url = f"https://dkxm8.com/?area={area_name}"
            else:
                url = f"https://dkxm8.com/?area={area_name}&page={page_num}"

            self.driver.get(url)
            time.sleep(2)

            # ì—…ì²´ ì¹´ë“œ ì°¾ê¸°
            cards = []
            selectors = ["div.gal_top", "div.gal_list", "div[class*='gal']"]

            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if len(elements) > 0:
                        cards = elements
                        break
                except:
                    continue

            if not cards:
                return items

            # ê° ì¹´ë“œì—ì„œ ì •ë³´ ì¶”ì¶œ
            for idx, card in enumerate(cards, 1):
                try:
                    # ì—…ì²´ëª…
                    title = ""
                    try:
                        h4 = card.find_element(By.TAG_NAME, "h4")
                        title = h4.text.strip()
                    except:
                        pass

                    # í”„ë¡œí•„ URL
                    profile_url = ""
                    try:
                        profile_link = card.find_element(By.CSS_SELECTOR, "a[href*='profile_popup']")
                        profile_url = profile_link.get_attribute('href')
                    except:
                        pass

                    # ì„¤ëª…
                    description = ""
                    try:
                        strong = card.find_element(By.CSS_SELECTOR, ".galInfo strong")
                        description = strong.text.strip()
                    except:
                        pass

                    # ì˜ì—…ì‹œê°„
                    hours = ""
                    try:
                        hour_elem = card.find_element(By.XPATH, ".//p[b[text()='ì˜ì—…ì‹œê°„']]/span")
                        hours = hour_elem.text.strip()
                    except:
                        pass

                    # ì „í™”ë²ˆí˜¸
                    phone = ""
                    try:
                        phone_elem = card.find_element(By.XPATH, ".//p[b[text()='ì „í™”ë²ˆí˜¸']]/span")
                        phone = phone_elem.text.strip()
                    except:
                        pass

                    # ì¹´í†¡ ID
                    kakao_id = ""
                    try:
                        kakao_elem = card.find_element(By.XPATH, ".//p[b[contains(text(),'ì¹´í†¡')]]/span")
                        kakao_id = kakao_elem.text.strip()
                    except:
                        pass

                    # í…”ë ˆê·¸ë¨ ID
                    telegram_id = ""
                    try:
                        telegram_elem = card.find_element(By.XPATH, ".//p[b[contains(text(),'í…”ë ˆ')]]/span")
                        telegram_id = telegram_elem.text.strip()
                    except:
                        pass

                    # ì¸ë„¤ì¼ ì´ë¯¸ì§€
                    image_url = ""
                    try:
                        img = card.find_element(By.CSS_SELECTOR, ".imgwrap img")
                        image_url = img.get_attribute('src')
                        if image_url and not image_url.startswith('http'):
                            image_url = 'https://dkxm8.com' + image_url
                    except:
                        pass

                    if title or profile_url:
                        item = {
                            'title': title,
                            'url': profile_url,
                            'description': description,
                            'hours': hours,
                            'phone': phone,
                            'kakao_id': kakao_id,
                            'telegram_id': telegram_id,
                            'thumbnail': image_url,
                            'area': area_name,
                            'region': region_name,
                            'page': page_num
                        }
                        items.append(item)

                except Exception as e:
                    continue

            return items

        except Exception as e:
            print(f"      âŒ í˜ì´ì§€ {page_num} ì˜¤ë¥˜: {str(e)[:30]}")
            return items

    def crawl_area_all_pages(self, area_name, region_name):
        """íŠ¹ì • ì§€ì—­ì˜ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§"""
        print(f"\n   ğŸ“„ {area_name} í¬ë¡¤ë§ ì¤‘...")

        all_items = []

        try:
            # ì²« í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
            url = f"https://dkxm8.com/?area={area_name}"
            self.driver.get(url)
            time.sleep(2)

            total_pages = self.get_total_pages(area_name)
            print(f"      ì´ {total_pages}ê°œ í˜ì´ì§€ ë°œê²¬")

            # ê° í˜ì´ì§€ í¬ë¡¤ë§
            for page_num in range(1, total_pages + 1):
                print(f"      í˜ì´ì§€ {page_num}/{total_pages}...", end=" ")

                items = self.crawl_page(area_name, region_name, page_num)
                all_items.extend(items)

                print(f"{len(items)}ê°œ")
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            print(f"   âœ… {area_name}: ì´ {len(all_items)}ê°œ ì—…ì²´ ìˆ˜ì§‘")
            return all_items

        except Exception as e:
            print(f"   âŒ {area_name} ì˜¤ë¥˜: {str(e)[:50]}")
            return all_items

    def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        print("=" * 70)
        if self.test_mode:
            print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ëŒ€ì „ë§Œ í¬ë¡¤ë§")
        else:
            print("ğŸš€ ì „ì²´ ì§€ì—­ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("=" * 70)

        try:
            if not self.setup_driver():
                return False

            if not self.login():
                return False

            regions = TEST_REGIONS if self.test_mode else ALL_REGIONS
            total_cities = sum(len(cities) for cities in regions.values())
            print(f"\nì „ì²´ ì§€ì—­: {len(regions)}ê°œ ê¶Œì—­, {total_cities}ê°œ ë„ì‹œ")

            current = 0
            for region_name, cities in regions.items():
                print(f"\n{'='*70}")
                print(f"ğŸ—ºï¸  {region_name} ({len(cities)}ê°œ ë„ì‹œ)")
                print(f"{'='*70}")

                for city in cities:
                    current += 1
                    print(f"[{current}/{total_cities}]", end="")
                    items = self.crawl_area_all_pages(city, region_name)
                    self.results.extend(items)
                    time.sleep(2)  # ë„ì‹œ ê°„ ëŒ€ê¸°

            # ê²°ê³¼ ì €ì¥
            mode_suffix = "test" if self.test_mode else "full"
            output_file = f'crawl_pagination_{mode_suffix}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)

            print("\n" + "=" * 70)
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(self.results)}ê°œ ì—…ì²´ ìˆ˜ì§‘")
            print(f"ğŸ’¾ íŒŒì¼ ì €ì¥: {output_file}")
            print("=" * 70)

            # í†µê³„ ì¶œë ¥
            print("\nğŸ“Š ê¶Œì—­ë³„ í†µê³„:")
            region_stats = {}
            for item in self.results:
                region = item.get('region', 'ë¯¸ë¶„ë¥˜')
                region_stats[region] = region_stats.get(region, 0) + 1

            for region, count in sorted(region_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"   {region}: {count}ê°œ")

            # ë„ì‹œë³„ í†µê³„
            print("\nğŸ“Š ë„ì‹œë³„ í†µê³„:")
            area_stats = {}
            for item in self.results:
                area = item.get('area', 'ë¯¸ë¶„ë¥˜')
                area_stats[area] = area_stats.get(area, 0) + 1

            for area, count in sorted(area_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"   {area}: {count}ê°œ")

            return True

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False

        finally:
            if self.driver:
                self.driver.quit()
                print("\nğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")


if __name__ == "__main__":
    import sys

    headless = '--headless' in sys.argv
    full_mode = '--full' in sys.argv
    test_mode = not full_mode

    if headless:
        print("â„¹ï¸  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤")
    else:
        print("â„¹ï¸  ì¼ë°˜ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤")

    if test_mode:
        print("â„¹ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ëŒ€ì „ë§Œ í¬ë¡¤ë§ (ì•½ 5ë¶„)")
    else:
        print("â„¹ï¸  ì „ì²´ ëª¨ë“œ: ì „êµ­ í¬ë¡¤ë§ (ì•½ 1-2ì‹œê°„)")

    crawler = PaginationCrawler(headless=headless, test_mode=test_mode)
    success = crawler.run()

    sys.exit(0 if success else 1)
