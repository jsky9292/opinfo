#!/usr/bin/env python3
"""
ë°œê²¬ëœ ëª¨ë“  ì§€ì—­(106ê°œ)ì„ ìˆœì°¨ í¬ë¡¤ë§
"""
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import time, json
from datetime import datetime

USERNAME = 'mix1220'
PASSWORD = '1234'

class CompleteCrawler:
    def __init__(self):
        self.driver = None
        self.results = []

    def setup(self):
        print('ë¸Œë¼ìš°ì € ì„¤ì •...')
        opts = Options()
        opts.add_argument('--no-sandbox')
        opts.add_argument('--disable-dev-shm-usage')
        opts.add_argument('--disable-blink-features=AutomationControlled')
        opts.add_argument('--headless')
        opts.add_argument('--window-size=1920,1080')
        opts.add_experimental_option("excludeSwitches", ["enable-automation"])
        opts.add_experimental_option('useAutomationExtension', False)
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
        print('ì¤€ë¹„ ì™„ë£Œ')

    def login(self):
        print('ë¡œê·¸ì¸...')
        self.driver.get('https://dkxm8.com/bbs/login.php')
        time.sleep(2)
        self.driver.find_element(By.NAME, 'mb_id').send_keys(USERNAME)
        self.driver.find_element(By.NAME, 'mb_password').send_keys(PASSWORD)
        self.driver.find_element(By.CSS_SELECTOR, 'input[type=submit]').click()
        time.sleep(3)
        print('ë¡œê·¸ì¸ ì™„ë£Œ\n')

    def crawl_area(self, area_name):
        """íŠ¹ì • ì§€ì—­ì˜ ëª¨ë“  ì—…ì†Œ í¬ë¡¤ë§ (ëŒ€ì „ ë°©ì‹)"""
        try:
            from urllib.parse import quote
            encoded_area = quote(area_name)
            url = f"https://dkxm8.com/?area={encoded_area}"
            self.driver.get(url)
            time.sleep(3)

            # ì—…ì²´ ì¹´ë“œ ì°¾ê¸°
            cards = []
            selectors = ["div.gal_top", "div.gal_list", "div[class*='gal']"]

            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if len(elements) > 0:
                        cards = elements
                        break
                except:
                    continue

            if not cards:
                return []

            items = []
            for idx, card in enumerate(cards, 1):
                try:
                    # ì—…ì²´ëª…
                    title = ""
                    try:
                        h4 = card.find_element(By.TAG_NAME, "h4")
                        title = h4.text.strip()
                    except:
                        pass

                    # URL
                    shop_url = ""
                    try:
                        profile_link = card.find_element(By.CSS_SELECTOR, "a[href*='profile_popup']")
                        shop_url = profile_link.get_attribute('href')
                    except:
                        pass

                    # ì„¤ëª…
                    description = ""
                    try:
                        strong = card.find_element(By.CSS_SELECTOR, ".galInfo strong")
                        description = strong.text.strip()
                    except:
                        pass

                    # ì˜ì—…ì‹œê°„
                    hours = ""
                    try:
                        hour_elem = card.find_element(By.XPATH, ".//p[b[text()='ì˜ì—…ì‹œê°„']]/span")
                        hours = hour_elem.text.strip()
                    except:
                        pass

                    # ì „í™”ë²ˆí˜¸
                    phone = ""
                    try:
                        phone_elem = card.find_element(By.XPATH, ".//p[b[text()='ì „í™”ë²ˆí˜¸']]/span")
                        phone = phone_elem.text.strip()
                    except:
                        pass

                    # ì¹´ì¹´ì˜¤í†¡
                    kakao_id = ""
                    try:
                        kakao_elem = card.find_element(By.XPATH, ".//p[b[contains(text(),'ì¹´í†¡')]]/span")
                        kakao_id = kakao_elem.text.strip()
                    except:
                        pass

                    # í…”ë ˆê·¸ë¨
                    telegram_id = ""
                    try:
                        telegram_elem = card.find_element(By.XPATH, ".//p[b[contains(text(),'í…”ë ˆ')]]/span")
                        telegram_id = telegram_elem.text.strip()
                    except:
                        pass

                    # ì¸ë„¤ì¼
                    thumbnail = ""
                    try:
                        img = card.find_element(By.CSS_SELECTOR, ".imgwrap img")
                        thumbnail = img.get_attribute('src')
                        if thumbnail and not thumbnail.startswith('http'):
                            thumbnail = 'https://dkxm8.com' + thumbnail
                        if not thumbnail:
                            thumbnail = 'https://dkxm8.com/img/temp_thum.jpg'
                    except:
                        thumbnail = 'https://dkxm8.com/img/temp_thum.jpg'

                    if title or shop_url:
                        item = {
                            'title': title,
                            'url': shop_url,
                            'description': description,
                            'hours': hours,
                            'phone': phone,
                            'kakao_id': kakao_id,
                            'telegram_id': telegram_id,
                            'thumbnail': thumbnail,
                            'area': area_name  # ì§€ì—­ ì •ë³´ ì¶”ê°€
                        }
                        items.append(item)

                except Exception as e:
                    continue

            return items

        except Exception as e:
            print(f'  âœ— ì˜¤ë¥˜: {e}')
            return []

    def run(self):
        print('='*60)
        print('ì „êµ­ 106ê°œ ì§€ì—­ ìˆœì°¨ í¬ë¡¤ë§')
        print('='*60)

        # ë°œê²¬ëœ ì§€ì—­ ëª©ë¡ ë¡œë“œ
        with open('all_areas.json', 'r', encoding='utf-8') as f:
            areas = json.load(f)

        print(f'\nì´ {len(areas)}ê°œ ì§€ì—­ í¬ë¡¤ë§ ì‹œì‘\n')

        self.setup()
        self.login()

        for idx, area in enumerate(areas, 1):
            print(f'[{idx}/{len(areas)}] {area:20s}', end=' ')

            items = self.crawl_area(area)

            if items:
                self.results.extend(items)
                # ì¸ë„¤ì¼ ì²´í¬
                real_thumbs = len([x for x in items if 'data/file/main' in x.get('thumbnail', '')])
                print(f'âœ“ {len(items):3d}ê°œ (ì¸ë„¤ì¼ {real_thumbs:3d}ê°œ)')
            else:
                print('â—‹ ì—†ìŒ')

            # 10ê°œ ì§€ì—­ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
            if idx % 10 == 0:
                temp_file = f'complete_crawl_temp_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=2)
                print(f'  â†’ ì¤‘ê°„ ì €ì¥: {temp_file} (ëˆ„ì  {len(self.results):,}ê°œ)\n')

            time.sleep(2)

        # ìµœì¢… ì €ì¥
        output = f'complete_crawl_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

        print(f'\n{"="*60}')
        print(f'âœ… ì™„ë£Œ: {len(self.results):,}ê°œ ì—…ì†Œ â†’ {output}')
        print(f'{"="*60}')

        # í†µê³„
        real_thumbs = len([x for x in self.results if 'data/file/main' in x.get('thumbnail', '')])
        with_phone = len([x for x in self.results if x.get('phone')])

        print(f'\nğŸ“Š í†µê³„:')
        print(f'  ì´ ì—…ì†Œ: {len(self.results):,}ê°œ')
        print(f'  ì‹¤ì œ ì¸ë„¤ì¼: {real_thumbs:,}ê°œ ({real_thumbs/len(self.results)*100:.1f}%)')
        print(f'  ì „í™”ë²ˆí˜¸: {with_phone:,}ê°œ ({with_phone/len(self.results)*100:.1f}%)')

        self.driver.quit()

if __name__ == '__main__':
    crawler = CompleteCrawler()
    crawler.run()
