#!/usr/bin/env python3
"""
dkxm8.com ì „ì²´ ì—…ì²´ ì™„ì „ í¬ë¡¤ë§ - ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
from datetime import datetime

# ë¡œê·¸ì¸ ì •ë³´
USERNAME = "mix1220"
PASSWORD = "1234"

# í…ŒìŠ¤íŠ¸ìš© - ëŒ€ì „ë§Œ í¬ë¡¤ë§
TEST_CITY = "ëŒ€ì „"

class CompleteCrawler:
    def __init__(self, headless=True):
        self.headless = headless
        self.driver = None
        self.results = []

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        print("ğŸ”§ ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")

        chrome_options = Options()

        if self.headless:
            chrome_options.add_argument('--headless=new')

        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)

        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.implicitly_wait(10)

        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ\n")

    def login(self):
        """ë¡œê·¸ì¸"""
        try:
            print("ğŸ”‘ ë¡œê·¸ì¸ ì¤‘...")
            self.driver.get("https://dkxm8.com/bbs/login.php")
            time.sleep(3)

            username_field = self.driver.find_element(By.NAME, "mb_id")
            password_field = self.driver.find_element(By.NAME, "mb_password")

            username_field.clear()
            username_field.send_keys(USERNAME)

            password_field.clear()
            password_field.send_keys(PASSWORD)

            submit_btn = self.driver.find_element(By.CSS_SELECTOR, "input[type='submit']")
            submit_btn.click()

            time.sleep(5)

            if 'login.php' in self.driver.current_url:
                print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨")
                return False

            print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!\n")
            return True

        except Exception as e:
            print(f"âŒ ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
            return False

    def get_total_pages(self):
        """ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""
        try:
            # í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œ ì°¾ê¸°
            pagination_selectors = [
                "div.pg_wrap",
                "div.pagination",
                "div[class*='paging']",
                "div[class*='page']",
            ]

            for selector in pagination_selectors:
                try:
                    pagination = self.driver.find_element(By.CSS_SELECTOR, selector)
                    # í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸°
                    page_links = pagination.find_elements(By.TAG_NAME, "a")

                    max_page = 1
                    for link in page_links:
                        try:
                            page_num = int(link.text.strip())
                            max_page = max(max_page, page_num)
                        except:
                            continue

                    if max_page > 1:
                        return max_page
                except:
                    continue

            # í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì§€ ëª»í•˜ë©´ 1í˜ì´ì§€ë¡œ ê°„ì£¼
            return 1

        except Exception as e:
            print(f"   âš ï¸ í˜ì´ì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            return 1

    def extract_shop_data(self, card):
        """ì—…ì²´ ì¹´ë“œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            data = {}

            # ì œëª©
            try:
                h4 = card.find_element(By.TAG_NAME, "h4")
                data['title'] = h4.text.strip()
            except:
                return None

            # URL
            try:
                link = card.find_element(By.CSS_SELECTOR, "a[href*='profile']")
                data['url'] = link.get_attribute('href')
            except:
                data['url'] = ""

            # ì„¤ëª…
            try:
                desc = card.find_element(By.CSS_SELECTOR, ".galInfo strong, .description")
                data['description'] = desc.text.strip()
            except:
                data['description'] = ""

            # ì˜ì—…ì‹œê°„
            try:
                hours = card.find_element(By.XPATH, ".//p[contains(., 'ì˜ì—…ì‹œê°„')]//span | .//span[contains(@class, 'hours')]")
                data['hours'] = hours.text.strip()
            except:
                data['hours'] = ""

            # ì „í™”ë²ˆí˜¸
            try:
                phone = card.find_element(By.XPATH, ".//a[contains(@href, 'tel:')] | .//p[contains(., 'ì „í™”')]//span")
                data['phone'] = phone.text.strip().replace('tel:', '')
            except:
                data['phone'] = ""

            # ì¸ë„¤ì¼ ì´ë¯¸ì§€
            try:
                img = card.find_element(By.TAG_NAME, "img")
                data['thumbnail'] = img.get_attribute('src')
            except:
                data['thumbnail'] = ""

            # ì¹´ì¹´ì˜¤í†¡ ID
            try:
                kakao = card.find_element(By.XPATH, ".//a[contains(@href, 'kakao') or contains(., 'ì¹´í†¡')]")
                data['kakao_id'] = kakao.text.strip()
            except:
                data['kakao_id'] = ""

            # í…”ë ˆê·¸ë¨ ID
            try:
                telegram = card.find_element(By.XPATH, ".//a[contains(@href, 't.me') or contains(., 'í…”ë ˆ')]")
                data['telegram_id'] = telegram.text.strip()
            except:
                data['telegram_id'] = ""

            return data

        except Exception as e:
            return None

    def crawl_page(self, url, page_num=1):
        """íŠ¹ì • í˜ì´ì§€ í¬ë¡¤ë§"""
        print(f"      í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘...", end=" ")

        items = []

        try:
            self.driver.get(url)
            time.sleep(3)

            # í˜ì´ì§€ ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

            # ì—…ì²´ ì¹´ë“œ ì°¾ê¸°
            cards = []
            selectors = ["div.gal_top", "div.gal_list", "div[class*='gal']"]

            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        for elem in elements:
                            if elem not in cards:
                                cards.append(elem)
                except:
                    pass

            if not cards:
                print("âš ï¸ ì—…ì²´ ì—†ìŒ")
                return items

            # ê° ì¹´ë“œ ì²˜ë¦¬
            for card in cards:
                data = self.extract_shop_data(card)
                if data and data.get('title'):
                    items.append(data)

            print(f"âœ“ {len(items)}ê°œ")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

        return items

    def crawl_city(self, city_name):
        """íŠ¹ì • ë„ì‹œ ì „ì²´ í¬ë¡¤ë§"""
        print(f"\n{'='*70}")
        print(f"ğŸ“ {city_name} í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*70}")

        all_items = []

        try:
            # ì²« í˜ì´ì§€ ì ‘ì†
            base_url = f"https://dkxm8.com/?area={city_name}"
            self.driver.get(base_url)
            time.sleep(3)

            # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
            total_pages = self.get_total_pages()
            print(f"   âœ“ ì „ì²´ í˜ì´ì§€ ìˆ˜: {total_pages}\n")

            # ê° í˜ì´ì§€ í¬ë¡¤ë§
            for page in range(1, total_pages + 1):
                if page == 1:
                    page_url = base_url
                else:
                    page_url = f"{base_url}&page={page}"

                items = self.crawl_page(page_url, page)
                all_items.extend(items)

                # í˜ì´ì§€ ê°„ ë”œë ˆì´
                if page < total_pages:
                    time.sleep(2)

            print(f"\n   âœ… {city_name}: ì´ {len(all_items)}ê°œ ì—…ì²´ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        return all_items

    def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            self.setup_driver()

            if not self.login():
                return

            print("\n" + "="*70)
            print("ğŸš€ ì™„ì „ í¬ë¡¤ëŸ¬ ì‹œì‘ (ëŒ€ì „ í…ŒìŠ¤íŠ¸)")
            print("="*70)

            # ëŒ€ì „ í¬ë¡¤ë§
            items = self.crawl_city(TEST_CITY)

            # ê²°ê³¼ ì €ì¥
            if items:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"complete_crawl_{TEST_CITY}_{timestamp}.json"

                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(items, f, ensure_ascii=False, indent=2)

                print("\n" + "="*70)
                print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(items)}ê°œ ì—…ì²´ ìˆ˜ì§‘")
                print(f"ğŸ’¾ íŒŒì¼ ì €ì¥: {filename}")
                print("="*70)
            else:
                print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

        finally:
            if self.driver:
                print("\nğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")
                self.driver.quit()

if __name__ == "__main__":
    print("â„¹ï¸  ì™„ì „ í¬ë¡¤ëŸ¬ - ëŒ€ì „ ì „ì²´ ì—…ì²´ ìˆ˜ì§‘")
    print("â„¹ï¸  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰\n")

    crawler = CompleteCrawler(headless=True)
    crawler.run()
