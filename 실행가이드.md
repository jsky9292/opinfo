# 🚀 dkxm8.com 크롤러 실행 가이드

## 📋 준비 사항

### 1. Python 설치 확인
```bash
python --version
# 또는
python3 --version
```
Python 3.7 이상 필요

### 2. Chrome 브라우저 설치
- Windows/Mac: https://www.google.com/chrome/
- 이미 설치되어 있으면 OK

---

## ⚡ 빠른 시작

### 1단계: 패키지 설치
```bash
pip install -r requirements.txt
```

### 2단계: 크롤러 실행
```bash
# 브라우저 보면서 실행 (권장 - 처음 테스트 시)
python dkxm8_crawler.py

# 백그라운드 실행 (UI 없음)
python dkxm8_crawler.py --headless
```

---

## 📁 생성되는 파일

실행 후 다음 파일들이 생성됩니다:

| 파일명 | 설명 |
|--------|------|
| `screenshot_main.png` | 메인 페이지 스크린샷 |
| `screenshot_after_login.png` | 로그인 후 스크린샷 |
| `page_main.html` | 메인 페이지 HTML 소스 |
| `crawl_results_YYYYMMDD_HHMMSS.json` | 크롤링 결과 (JSON) |
| `categories_YYYYMMDD_HHMMSS.csv` | 카테고리 목록 (엑셀에서 열기 가능) |

---

## 🔧 문제 해결

### 문제 1: "ModuleNotFoundError: No module named 'selenium'"
```bash
pip install selenium webdriver-manager
```

### 문제 2: ChromeDriver 관련 오류
- 자동으로 다운로드됩니다. 인터넷 연결을 확인하세요.
- 또는 수동 설치: https://chromedriver.chromium.org/

### 문제 3: 로그인 실패
1. `screenshot_after_login.png` 파일을 확인
2. 아이디/비밀번호 확인 필요
3. 코드 내 `USERNAME`, `PASSWORD` 변수 수정

### 문제 4: 카테고리를 찾지 못함
- 로그인 후 페이지 구조가 다를 수 있음
- `page_main.html`을 브라우저에서 열어 구조 확인
- 필요시 셀렉터 수정

---

## 🎯 다음 단계: 코드 커스터마이징

### 특정 데이터 추출하기

크롤러의 `crawl_category()` 메서드를 수정하여 원하는 데이터를 추출할 수 있습니다.

#### 예시 1: 게시글 제목과 링크
```python
def crawl_category(self, category_url, category_name):
    self.driver.get(category_url)
    time.sleep(3)
    
    items = []
    # 게시글 목록 (실제 셀렉터로 변경 필요)
    posts = self.driver.find_elements(By.CSS_SELECTOR, ".post-item")
    
    for post in posts:
        try:
            title = post.find_element(By.CSS_SELECTOR, ".title").text
            link = post.find_element(By.CSS_SELECTOR, "a").get_attribute('href')
            author = post.find_element(By.CSS_SELECTOR, ".author").text
            
            items.append({
                'title': title,
                'link': link,
                'author': author,
                'category': category_name
            })
        except:
            continue
    
    return items
```

#### 예시 2: 상품 정보
```python
def crawl_category(self, category_url, category_name):
    self.driver.get(category_url)
    time.sleep(3)
    
    items = []
    products = self.driver.find_elements(By.CSS_SELECTOR, ".product-item")
    
    for product in products:
        try:
            name = product.find_element(By.CSS_SELECTOR, ".product-name").text
            price = product.find_element(By.CSS_SELECTOR, ".price").text
            image = product.find_element(By.CSS_SELECTOR, "img").get_attribute('src')
            
            items.append({
                'name': name,
                'price': price,
                'image': image,
                'category': category_name
            })
        except:
            continue
    
    return items
```

### 셀렉터 찾는 방법

1. Chrome에서 사이트 접속
2. F12 (개발자 도구) 열기
3. Elements 탭에서 원하는 요소 찾기
4. 우클릭 → Copy → Copy selector

---

## 💾 데이터베이스 저장

### SQLite 저장 예시
```python
import sqlite3

def save_to_db(self, items):
    conn = sqlite3.connect('dkxm8_data.db')
    cursor = conn.cursor()
    
    # 테이블 생성
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            url TEXT,
            category TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # 데이터 삽입
    for item in items:
        cursor.execute(
            'INSERT INTO items (title, url, category) VALUES (?, ?, ?)',
            (item['title'], item['url'], item['category'])
        )
    
    conn.commit()
    conn.close()
```

### MySQL 저장 예시
```python
import pymysql

def save_to_mysql(self, items):
    conn = pymysql.connect(
        host='localhost',
        user='your_user',
        password='your_password',
        database='your_database',
        charset='utf8mb4'
    )
    
    cursor = conn.cursor()
    
    for item in items:
        cursor.execute(
            'INSERT INTO items (title, url, category) VALUES (%s, %s, %s)',
            (item['title'], item['url'], item['category'])
        )
    
    conn.commit()
    conn.close()
```

---

## 🔄 자동화 스케줄링

### Windows - 작업 스케줄러
1. 작업 스케줄러 실행
2. "기본 작업 만들기"
3. 트리거: 매일/매주 설정
4. 동작: 프로그램 시작
   - 프로그램: `python`
   - 인수: `C:\경로\dkxm8_crawler.py --headless`

### Mac/Linux - Cron
```bash
# crontab 편집
crontab -e

# 매일 오전 9시 실행
0 9 * * * cd /path/to/crawler && python3 dkxm8_crawler.py --headless

# 매시간 실행
0 * * * * cd /path/to/crawler && python3 dkxm8_crawler.py --headless
```

---

## 📞 추가 도움

추가로 필요한 기능이나 문제가 있으면 말씀해주세요:
- 특정 데이터 추출 로직
- 데이터베이스 연동
- 에러 처리 강화
- 성능 최적화
- 대량 크롤링 전략

---

## ⚠️ 주의사항

1. **법적 책임**: 크롤링 전 사이트 이용약관 확인
2. **속도 제한**: 요청 간 충분한 딜레이 (현재 3-5초)
3. **IP 차단**: 과도한 요청 시 IP 차단 가능
4. **개인정보**: 수집 데이터에 개인정보 포함 시 관련 법규 준수
5. **서버 부하**: 트래픽이 많은 시간대 피하기

---

## 📊 성능 팁

### 병렬 처리 (고급)
```python
from concurrent.futures import ThreadPoolExecutor

def crawl_all_categories(self, categories):
    with ThreadPoolExecutor(max_workers=3) as executor:
        results = executor.map(
            lambda cat: self.crawl_category(cat['url'], cat['text']),
            categories
        )
    return list(results)
```

### 프록시 사용 (IP 차단 방지)
```python
chrome_options.add_argument('--proxy-server=http://proxy_ip:port')
```

### 요청 속도 조절
```python
import random
time.sleep(random.uniform(2, 5))  # 2-5초 랜덤 대기
```

---

**버전**: 1.0  
**최종 수정**: 2025-10-08  
**로그인 정보**: mdmix / 1234
