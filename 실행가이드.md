# ğŸš€ dkxm8.com í¬ë¡¤ëŸ¬ ì‹¤í–‰ ê°€ì´ë“œ

## ğŸ“‹ ì¤€ë¹„ ì‚¬í•­

### 1. Python ì„¤ì¹˜ í™•ì¸
```bash
python --version
# ë˜ëŠ”
python3 --version
```
Python 3.7 ì´ìƒ í•„ìš”

### 2. Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜
- Windows/Mac: https://www.google.com/chrome/
- ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ OK

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 2ë‹¨ê³„: í¬ë¡¤ëŸ¬ ì‹¤í–‰
```bash
# ë¸Œë¼ìš°ì € ë³´ë©´ì„œ ì‹¤í–‰ (ê¶Œì¥ - ì²˜ìŒ í…ŒìŠ¤íŠ¸ ì‹œ)
python dkxm8_crawler.py

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (UI ì—†ìŒ)
python dkxm8_crawler.py --headless
```

---

## ğŸ“ ìƒì„±ë˜ëŠ” íŒŒì¼

ì‹¤í–‰ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

| íŒŒì¼ëª… | ì„¤ëª… |
|--------|------|
| `screenshot_main.png` | ë©”ì¸ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· |
| `screenshot_after_login.png` | ë¡œê·¸ì¸ í›„ ìŠ¤í¬ë¦°ìƒ· |
| `page_main.html` | ë©”ì¸ í˜ì´ì§€ HTML ì†ŒìŠ¤ |
| `crawl_results_YYYYMMDD_HHMMSS.json` | í¬ë¡¤ë§ ê²°ê³¼ (JSON) |
| `categories_YYYYMMDD_HHMMSS.csv` | ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ì—‘ì…€ì—ì„œ ì—´ê¸° ê°€ëŠ¥) |

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: "ModuleNotFoundError: No module named 'selenium'"
```bash
pip install selenium webdriver-manager
```

### ë¬¸ì œ 2: ChromeDriver ê´€ë ¨ ì˜¤ë¥˜
- ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.
- ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜: https://chromedriver.chromium.org/

### ë¬¸ì œ 3: ë¡œê·¸ì¸ ì‹¤íŒ¨
1. `screenshot_after_login.png` íŒŒì¼ì„ í™•ì¸
2. ì•„ì´ë””/ë¹„ë°€ë²ˆí˜¸ í™•ì¸ í•„ìš”
3. ì½”ë“œ ë‚´ `USERNAME`, `PASSWORD` ë³€ìˆ˜ ìˆ˜ì •

### ë¬¸ì œ 4: ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì§€ ëª»í•¨
- ë¡œê·¸ì¸ í›„ í˜ì´ì§€ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- `page_main.html`ì„ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ êµ¬ì¡° í™•ì¸
- í•„ìš”ì‹œ ì…€ë ‰í„° ìˆ˜ì •

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ì½”ë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### íŠ¹ì • ë°ì´í„° ì¶”ì¶œí•˜ê¸°

í¬ë¡¤ëŸ¬ì˜ `crawl_category()` ë©”ì„œë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ì˜ˆì‹œ 1: ê²Œì‹œê¸€ ì œëª©ê³¼ ë§í¬
```python
def crawl_category(self, category_url, category_name):
    self.driver.get(category_url)
    time.sleep(3)
    
    items = []
    # ê²Œì‹œê¸€ ëª©ë¡ (ì‹¤ì œ ì…€ë ‰í„°ë¡œ ë³€ê²½ í•„ìš”)
    posts = self.driver.find_elements(By.CSS_SELECTOR, ".post-item")
    
    for post in posts:
        try:
            title = post.find_element(By.CSS_SELECTOR, ".title").text
            link = post.find_element(By.CSS_SELECTOR, "a").get_attribute('href')
            author = post.find_element(By.CSS_SELECTOR, ".author").text
            
            items.append({
                'title': title,
                'link': link,
                'author': author,
                'category': category_name
            })
        except:
            continue
    
    return items
```

#### ì˜ˆì‹œ 2: ìƒí’ˆ ì •ë³´
```python
def crawl_category(self, category_url, category_name):
    self.driver.get(category_url)
    time.sleep(3)
    
    items = []
    products = self.driver.find_elements(By.CSS_SELECTOR, ".product-item")
    
    for product in products:
        try:
            name = product.find_element(By.CSS_SELECTOR, ".product-name").text
            price = product.find_element(By.CSS_SELECTOR, ".price").text
            image = product.find_element(By.CSS_SELECTOR, "img").get_attribute('src')
            
            items.append({
                'name': name,
                'price': price,
                'image': image,
                'category': category_name
            })
        except:
            continue
    
    return items
```

### ì…€ë ‰í„° ì°¾ëŠ” ë°©ë²•

1. Chromeì—ì„œ ì‚¬ì´íŠ¸ ì ‘ì†
2. F12 (ê°œë°œì ë„êµ¬) ì—´ê¸°
3. Elements íƒ­ì—ì„œ ì›í•˜ëŠ” ìš”ì†Œ ì°¾ê¸°
4. ìš°í´ë¦­ â†’ Copy â†’ Copy selector

---

## ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥

### SQLite ì €ì¥ ì˜ˆì‹œ
```python
import sqlite3

def save_to_db(self, items):
    conn = sqlite3.connect('dkxm8_data.db')
    cursor = conn.cursor()
    
    # í…Œì´ë¸” ìƒì„±
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            url TEXT,
            category TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # ë°ì´í„° ì‚½ì…
    for item in items:
        cursor.execute(
            'INSERT INTO items (title, url, category) VALUES (?, ?, ?)',
            (item['title'], item['url'], item['category'])
        )
    
    conn.commit()
    conn.close()
```

### MySQL ì €ì¥ ì˜ˆì‹œ
```python
import pymysql

def save_to_mysql(self, items):
    conn = pymysql.connect(
        host='localhost',
        user='your_user',
        password='your_password',
        database='your_database',
        charset='utf8mb4'
    )
    
    cursor = conn.cursor()
    
    for item in items:
        cursor.execute(
            'INSERT INTO items (title, url, category) VALUES (%s, %s, %s)',
            (item['title'], item['url'], item['category'])
        )
    
    conn.commit()
    conn.close()
```

---

## ğŸ”„ ìë™í™” ìŠ¤ì¼€ì¤„ë§

### Windows - ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬
1. ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
2. "ê¸°ë³¸ ì‘ì—… ë§Œë“¤ê¸°"
3. íŠ¸ë¦¬ê±°: ë§¤ì¼/ë§¤ì£¼ ì„¤ì •
4. ë™ì‘: í”„ë¡œê·¸ë¨ ì‹œì‘
   - í”„ë¡œê·¸ë¨: `python`
   - ì¸ìˆ˜: `C:\ê²½ë¡œ\dkxm8_crawler.py --headless`

### Mac/Linux - Cron
```bash
# crontab í¸ì§‘
crontab -e

# ë§¤ì¼ ì˜¤ì „ 9ì‹œ ì‹¤í–‰
0 9 * * * cd /path/to/crawler && python3 dkxm8_crawler.py --headless

# ë§¤ì‹œê°„ ì‹¤í–‰
0 * * * * cd /path/to/crawler && python3 dkxm8_crawler.py --headless
```

---

## ğŸ“ ì¶”ê°€ ë„ì›€

ì¶”ê°€ë¡œ í•„ìš”í•œ ê¸°ëŠ¥ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”:
- íŠ¹ì • ë°ì´í„° ì¶”ì¶œ ë¡œì§
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
- ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
- ì„±ëŠ¥ ìµœì í™”
- ëŒ€ëŸ‰ í¬ë¡¤ë§ ì „ëµ

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë²•ì  ì±…ì„**: í¬ë¡¤ë§ ì „ ì‚¬ì´íŠ¸ ì´ìš©ì•½ê´€ í™•ì¸
2. **ì†ë„ ì œí•œ**: ìš”ì²­ ê°„ ì¶©ë¶„í•œ ë”œë ˆì´ (í˜„ì¬ 3-5ì´ˆ)
3. **IP ì°¨ë‹¨**: ê³¼ë„í•œ ìš”ì²­ ì‹œ IP ì°¨ë‹¨ ê°€ëŠ¥
4. **ê°œì¸ì •ë³´**: ìˆ˜ì§‘ ë°ì´í„°ì— ê°œì¸ì •ë³´ í¬í•¨ ì‹œ ê´€ë ¨ ë²•ê·œ ì¤€ìˆ˜
5. **ì„œë²„ ë¶€í•˜**: íŠ¸ë˜í”½ì´ ë§ì€ ì‹œê°„ëŒ€ í”¼í•˜ê¸°

---

## ğŸ“Š ì„±ëŠ¥ íŒ

### ë³‘ë ¬ ì²˜ë¦¬ (ê³ ê¸‰)
```python
from concurrent.futures import ThreadPoolExecutor

def crawl_all_categories(self, categories):
    with ThreadPoolExecutor(max_workers=3) as executor:
        results = executor.map(
            lambda cat: self.crawl_category(cat['url'], cat['text']),
            categories
        )
    return list(results)
```

### í”„ë¡ì‹œ ì‚¬ìš© (IP ì°¨ë‹¨ ë°©ì§€)
```python
chrome_options.add_argument('--proxy-server=http://proxy_ip:port')
```

### ìš”ì²­ ì†ë„ ì¡°ì ˆ
```python
import random
time.sleep(random.uniform(2, 5))  # 2-5ì´ˆ ëœë¤ ëŒ€ê¸°
```

---

**ë²„ì „**: 1.0  
**ìµœì¢… ìˆ˜ì •**: 2025-10-08  
**ë¡œê·¸ì¸ ì •ë³´**: mdmix / 1234
